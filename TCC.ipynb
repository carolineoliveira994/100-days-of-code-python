{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNMMdKK4b6feYZhtJOwaAj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "!pip install python-dotenv\n",
        "!pip install evaluate!pip install datasets\n",
        "!pip install transformers torch\n",
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "xmc7G_a_wTcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "id": "ySyla9YpJiMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Jbszt9ZoqtV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTiJrTIbOKJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import evaluate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "print(\"Diretório atual:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "metric = evaluate.load('f1')\n",
        "\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "ZkVmVkp4wWd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Arquivos no diretório atual:\", os.listdir())\n"
      ],
      "metadata": {
        "id": "VLNsh3zwqQ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "BLUESKY_APP_USER = 'cocorolini.bsky.social'\n",
        "BLUESKY_APP_PASS='3ovj-gbnh-trwd-k66r'\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "YYunxEJnqSs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py search BOULOS --sort latest --limit 90\n"
      ],
      "metadata": {
        "id": "NcKekFBqqVM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# Defina o caminho correto para o arquivo CSV\n",
        "file_path = './data/search_results_BOULOS_2024_09_19.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Cti5hDo8qkqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo de língua portuguesa\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Função de pré-processamento usando spaCy\n",
        "def preprocess_text_spacy(text_at):\n",
        "    # Aplicar o modelo spaCy ao texto\n",
        "    doc = nlp(text_at)\n",
        "    # Extrair palavras lematizadas e remover stopwords e pontuação\n",
        "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar pré-processamento ao DataFrame\n",
        "df['text_at'] = df['text'].apply(preprocess_text_spacy)  # Substitua 'post_content' pelo nome real da coluna\n"
      ],
      "metadata": {
        "id": "LIH2YhaqsE3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'text_at']].head())\n"
      ],
      "metadata": {
        "id": "NhrqBK5lsM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./data/search_results_BOULOS_2024_09_19.csv')\n"
      ],
      "metadata": {
        "id": "o-zAyb26suIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pYGeAdSOzbC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "c28hT73jssTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remover URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remover menções (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remover hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Remover números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remover pontuação e caracteres especiais\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remover múltiplos espaços em branco\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "va822KgpKbrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizer para o modelo BERT em português\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n"
      ],
      "metadata": {
        "id": "ewhB0M30KY1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"reply_count\", \"author_display_name\", \"author_handle\", \"indexed_at\", \"cid\", \"uri\", \"created_at\", \"repost_count\", \"like_count\"]\n",
        "df = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Exibir DataFrame após remover colunas\n",
        "print(\"\\nDepois:\")\n",
        "print(df)\n",
        ""
      ],
      "metadata": {
        "id": "ofX3gVGXM1hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo em português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Função para pré-processamento de texto\n",
        "def preprocess_text_spacy(text_at):\n",
        "    # Aplicar o modelo spaCy ao texto\n",
        "    doc = nlp(text_at)\n",
        "    # Extrair palavras lematizadas e remover stopwords e pontuação\n",
        "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar o pré-processamento ao DataFrame\n",
        "df['text_at'] = df['text'].apply(preprocess_text_spacy)\n"
      ],
      "metadata": {
        "id": "3p7POefYNAJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bWACLUvLOHcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessário para o VADER\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Criar um analisador de sentimento\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Assumindo que 'df' é seu DataFrame e 'text-at' é a coluna de texto\n",
        "def calcular_sentimento(texto):\n",
        "    score = sia.polarity_scores(texto)['compound']\n",
        "    return 'positivo' if score > 0 else 'negativo'\n",
        "\n",
        "# Aplicar a função a cada linha da coluna 'text-at' do DataFrame\n",
        "df['label'] = df['text_at'].apply(calcular_sentimento)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "n6WG4UT7SKKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'positivo': 1,\n",
        "    'negativo': 0\n",
        "}\n",
        "\n",
        "# Aplicar a conversão\n",
        "df['label'] = df['label'].map(label_mapping)\n"
      ],
      "metadata": {
        "id": "BeVvk1xyUfC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].unique())  # Deve mostrar apenas números, como [0, 1]\n"
      ],
      "metadata": {
        "id": "fyznQMhKUU2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Carregar o tokenizador do BERT em português\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Tokenizar os textos já pré-processados\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Aplicar a tokenização aos textos pré-processados\n",
        "df['tokenized'] = df['text_at'].apply(tokenize_text)\n",
        "\n",
        "# Exibir um exemplo tokenizado\n",
        "print(df['tokenized'].iloc[0])\n"
      ],
      "metadata": {
        "id": "E8Ba0R_GNB1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Criar novas colunas no DataFrame extraindo as informações do tokenized\n",
        "df['input_ids'] = df['tokenized'].apply(lambda x: x['input_ids'])\n",
        "df['attention_mask'] = df['tokenized'].apply(lambda x: x['attention_mask'])\n",
        "\n",
        "# Agora o DataFrame tem colunas de listas simples, que podem ser convertidas para um Dataset\n",
        "df_simplificado = df[['input_ids', 'attention_mask', 'label']]  # 'label' é sua coluna de rótulos\n",
        "\n",
        "# Converter para o Dataset do Hugging Face\n",
        "dataset = Dataset.from_pandas(df_simplificado)\n"
      ],
      "metadata": {
        "id": "Wx9My4ifNfr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Carregar o modelo BERT pré-treinado para classificação de sentimentos\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)  # 'num_labels' depende do número de classes (ex: 2 para positivo/negativo)\n",
        "\n",
        "# Configurar os argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',           # Diretório para salvar os resultados\n",
        "    num_train_epochs=3,               # Número de épocas\n",
        "    per_device_train_batch_size=8,    # Tamanho do batch por dispositivo durante o treinamento\n",
        "    per_device_eval_batch_size=16,    # Tamanho do batch por dispositivo durante a avaliação\n",
        "    evaluation_strategy=\"epoch\",      # Avaliação a cada época\n",
        "    save_total_limit=2,               # Limitar o número de checkpoints salvos\n",
        ")\n",
        "\n",
        "# Definir o treinador\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,            # Dataset de treinamento\n",
        "    eval_dataset=dataset,             # Dataset de avaliação\n",
        ")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "1-fKl_VINE1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "PE0hXhj6y4vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Exibir resultados\n",
        "print(f\"Resultados da avaliação: {eval_results}\")\n"
      ],
      "metadata": {
        "id": "DYQlyriG0UbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer previsões\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Exibir as predições\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "a61bLcjUXRIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Obtenha as predições e os rótulos verdadeiros\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Calcular o relatório de classificação\n",
        "report = classification_report(labels, preds, target_names=['negativo', 'positivo'])\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "id": "x-k5ZWx8XUQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenização\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Supondo que você já tenha um DataFrame `df` com as colunas 'text' e 'label'\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128  # Defina um tamanho máximo para padding e truncamento\n",
        "    )\n",
        "\n",
        "# Supondo um DataFrame com mais amostras\n",
        "df = pd.DataFrame({\n",
        "    'text': [\"Seu texto aqui\"] * 100,  # Adicione textos suficientes\n",
        "    'label': [0] * 100  # Adicione rótulos suficientes\n",
        "})\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Dividindo os dados em treino e validação\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "# Carregar métricas\n",
        "metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "# Definir os parâmetros de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    remove_unused_columns=False  # Adicionando esta linha para evitar erros de coluna\n",
        ")\n",
        "\n",
        "# Inicializar o modelo BERT\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
        "\n",
        "# Treinamento\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "uVsHyDxltpqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}