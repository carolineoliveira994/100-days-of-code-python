{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "!pip install python-dotenv\n",
        "!pip install evaluate!pip install datasets\n",
        "!pip install transformers torch\n",
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "xmc7G_a_wTcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "id": "ySyla9YpJiMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Jbszt9ZoqtV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTiJrTIbOKJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import evaluate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "print(\"Diretório atual:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "metric = evaluate.load('f1')\n",
        "\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "ZkVmVkp4wWd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Arquivos no diretório atual:\", os.listdir())\n"
      ],
      "metadata": {
        "id": "VLNsh3zwqQ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "BLUESKY_APP_USER = 'cocorolini.bsky.social'\n",
        "BLUESKY_APP_PASS='3ovj-gbnh-trwd-k66r'\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "YYunxEJnqSs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py search BOULOS --sort latest --limit 90\n"
      ],
      "metadata": {
        "id": "NcKekFBqqVM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# Defina o caminho correto para o arquivo CSV\n",
        "file_path = './data/search_results_BOULOS_2024_09_19.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Cti5hDo8qkqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo de língua portuguesa\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Função de pré-processamento usando spaCy\n",
        "def preprocess_text_spacy(text_at):\n",
        "    # Aplicar o modelo spaCy ao texto\n",
        "    doc = nlp(text_at)\n",
        "    # Extrair palavras lematizadas e remover stopwords e pontuação\n",
        "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar pré-processamento ao DataFrame\n",
        "df['text_at'] = df['text'].apply(preprocess_text_spacy)  # Substitua 'post_content' pelo nome real da coluna\n"
      ],
      "metadata": {
        "id": "LIH2YhaqsE3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'text_at']].head())\n"
      ],
      "metadata": {
        "id": "NhrqBK5lsM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./data/search_results_BOULOS_2024_09_19.csv')\n"
      ],
      "metadata": {
        "id": "o-zAyb26suIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pYGeAdSOzbC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "c28hT73jssTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remover URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remover menções (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remover hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Remover números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remover pontuação e caracteres especiais\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remover múltiplos espaços em branco\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "va822KgpKbrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizer para o modelo BERT em português\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n"
      ],
      "metadata": {
        "id": "ewhB0M30KY1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"reply_count\", \"author_display_name\", \"author_handle\", \"indexed_at\", \"cid\", \"uri\", \"created_at\", \"repost_count\", \"like_count\"]\n",
        "df = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Exibir DataFrame após remover colunas\n",
        "print(\"\\nDepois:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "ofX3gVGXM1hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo em português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Função para pré-processamento de texto\n",
        "def preprocess_text_spacy(text_at):\n",
        "    # Aplicar o modelo spaCy ao texto\n",
        "    doc = nlp(text_at)\n",
        "    # Extrair palavras lematizadas e remover stopwords e pontuação\n",
        "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar o pré-processamento ao DataFrame\n",
        "df['text_at'] = df['text'].apply(preprocess_text_spacy)\n"
      ],
      "metadata": {
        "id": "3p7POefYNAJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bWACLUvLOHcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessário para o VADER\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Criar um analisador de sentimento\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Assumindo que 'df' é seu DataFrame e 'text-at' é a coluna de texto\n",
        "def calcular_sentimento(texto):\n",
        "    score = sia.polarity_scores(texto)['compound']\n",
        "    return 'positivo' if score > 0 else 'negativo'\n",
        "\n",
        "# Aplicar a função a cada linha da coluna 'text-at' do DataFrame\n",
        "df['label'] = df['text_at'].apply(calcular_sentimento)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "n6WG4UT7SKKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'positivo': 1,\n",
        "    'negativo': 0\n",
        "}\n",
        "\n",
        "# Aplicar a conversão\n",
        "df['label'] = df['label'].map(label_mapping)\n"
      ],
      "metadata": {
        "id": "BeVvk1xyUfC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].unique())  # Deve mostrar apenas números, como [0, 1]\n"
      ],
      "metadata": {
        "id": "fyznQMhKUU2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Carregar o tokenizador do BERT em português\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Tokenizar os textos já pré-processados\n",
        "def tokenize_text(text):\n",
        "    return tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Aplicar a tokenização aos textos pré-processados\n",
        "df['tokenized'] = df['text_at'].apply(tokenize_text)\n",
        "\n",
        "# Exibir um exemplo tokenizado\n",
        "print(df['tokenized'].iloc[0])\n"
      ],
      "metadata": {
        "id": "E8Ba0R_GNB1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Criar novas colunas no DataFrame extraindo as informações do tokenized\n",
        "df['input_ids'] = df['tokenized'].apply(lambda x: x['input_ids'])\n",
        "df['attention_mask'] = df['tokenized'].apply(lambda x: x['attention_mask'])\n",
        "\n",
        "# Agora o DataFrame tem colunas de listas simples, que podem ser convertidas para um Dataset\n",
        "df_simplificado = df[['input_ids', 'attention_mask', 'label']]  # 'label' é sua coluna de rótulos\n",
        "\n",
        "# Converter para o Dataset do Hugging Face\n",
        "dataset = Dataset.from_pandas(df_simplificado)\n"
      ],
      "metadata": {
        "id": "Wx9My4ifNfr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Carregar o modelo BERT pré-treinado para classificação de sentimentos\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)  # 'num_labels' depende do número de classes (ex: 2 para positivo/negativo)\n",
        "\n",
        "# Configurar os argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',           # Diretório para salvar os resultados\n",
        "    num_train_epochs=3,               # Número de épocas\n",
        "    per_device_train_batch_size=8,    # Tamanho do batch por dispositivo durante o treinamento\n",
        "    per_device_eval_batch_size=16,    # Tamanho do batch por dispositivo durante a avaliação\n",
        "    evaluation_strategy=\"epoch\",      # Avaliação a cada época\n",
        "    save_total_limit=2,               # Limitar o número de checkpoints salvos\n",
        ")\n",
        "\n",
        "# Definir o treinador\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,            # Dataset de treinamento\n",
        "    eval_dataset=dataset,             # Dataset de avaliação\n",
        ")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "1-fKl_VINE1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96XmMreHaO4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Exibir resultados\n",
        "print(f\"Resultados da avaliação: {eval_results}\")\n"
      ],
      "metadata": {
        "id": "DYQlyriG0UbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWlGI5m8Y8oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer previsões\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Exibir as predições\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "a61bLcjUXRIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Obtenha as predições e os rótulos verdadeiros\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Calcular o relatório de classificação\n",
        "report = classification_report(labels, preds, target_names=['negativo', 'positivo'])\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "id": "x-k5ZWx8XUQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py search GUILHERMEBOULOS --sort latest --limit 90\n"
      ],
      "metadata": {
        "id": "bK8fnQ6HYGbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# Defina o caminho correto para o arquivo CSV\n",
        "file_path = './data/search_results_GUILHERMEBOULOS_2024_09_19.csv'\n",
        "df_test = pd.read_csv(file_path)\n",
        "\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "Zh1Fa9QtYURG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_text(text):\n",
        "    positive_keywords = ['ótimo', 'bom', 'maravilhoso', 'adoro', 'excelente', 'Prefeito', 'incrível', 'demais', 'casa comigo', 'delicia', 'Boulos50', 'boulos50', 'vitória', 'Vitória', 'PREFEITO', 'VITÓRIA', 'VITORIA', 'proposta', 'conhece']\n",
        "    negative_keywords = ['ruim', 'horrível', 'pior', 'não recomendo']\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    if any(word in text for word in positive_keywords):\n",
        "        return 1  # Positivo\n",
        "    elif any(word in text for word in negative_keywords):\n",
        "        return 0  # Negativo\n",
        "    else:\n",
        "        return -1  # Neutro ou indefinido\n",
        "\n",
        "# Aplicar a função para rotular automaticamente os textos\n",
        "df_test['label'] = df_test['text'].apply(label_text)\n",
        "\n",
        "# Exibir o DataFrame com os labels adicionados\n",
        "print(df_test)\n"
      ],
      "metadata": {
        "id": "dlWRgzLmYnet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o tokenizador e o modelo\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Função para rotular automaticamente os textos\n",
        "def label_with_model(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
        "    return predictions\n",
        "\n",
        "# Aplicar a função de rotulagem automática para todos os textos\n",
        "df_test['label'] = label_with_model(df_test['text'].tolist())\n",
        "\n",
        "# Exibir o DataFrame com os labels gerados automaticamente\n",
        "print(df_test)\n"
      ],
      "metadata": {
        "id": "LRh-zUBaa4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o tokenizador e o modelo\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Função para rotular automaticamente os textos\n",
        "def label_with_model(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
        "    return predictions\n",
        "\n",
        "# Aplicar a função de rotulagem automática para todos os textos\n",
        "df_test['label'] = label_with_model(df_test['text'].tolist())\n",
        "\n",
        "# Exibir o DataFrame com os labels gerados automaticamente\n",
        "print(df_test)\n"
      ],
      "metadata": {
        "id": "Ma4Md8Q_hnRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "VwaFKj-BhrUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separar o conjunto de dados em treino (80%) e teste (20%)\n",
        "train_df, test_df = train_test_split(df_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# Exibir as primeiras linhas dos dados de treino e teste\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n"
      ],
      "metadata": {
        "id": "IUhwul05iJRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Inicializar o tokenizador\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Tokenizar os textos de treinamento e teste\n",
        "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "ycuz86k4iLCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Criar datasets de treino e teste\n",
        "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
        "test_dataset = CustomDataset(test_encodings, test_df['label'].tolist())\n"
      ],
      "metadata": {
        "id": "aGI43slAiMyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Criar datasets de treino e teste\n",
        "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
        "test_dataset = CustomDataset(test_encodings, test_df['label'].tolist())\n"
      ],
      "metadata": {
        "id": "HOhf6LwziOGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Carregar o modelo BERT para classificação\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
        "\n",
        "# Definir os argumentos do treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Pasta para salvar os resultados\n",
        "    evaluation_strategy=\"epoch\",     # Avaliar ao final de cada época\n",
        "    per_device_train_batch_size=16,  # Tamanho do lote\n",
        "    per_device_eval_batch_size=16,   # Tamanho do lote para avaliação\n",
        "    num_train_epochs=3,              # Número de épocas\n",
        "    weight_decay=0.01,               # Taxa de decaimento\n",
        ")\n",
        "\n",
        "# Inicializar o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "pdbgDtemiPoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o modelo no conjunto de teste\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Exibir os resultados da avaliação\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "id": "78qVz5VUiU2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer previsões no conjunto de teste\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Exibir as previsões\n",
        "print(predictions.predictions)\n"
      ],
      "metadata": {
        "id": "D2h4goKLiWt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./modelo_final')\n",
        "tokenizer.save_pretrained('./modelo_final')\n"
      ],
      "metadata": {
        "id": "uMjZYSxoiYAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./meu_modelo')\n",
        "tokenizer.save_pretrained('./meu_modelo')\n"
      ],
      "metadata": {
        "id": "FsuWdG9ykBli"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}