{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP+AirzSmoAKCs7zkOAxgUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTiJrTIbOKJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"Diretório atual:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_metric\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "ZkVmVkp4wWd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n"
      ],
      "metadata": {
        "id": "xmc7G_a_wTcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv\n"
      ],
      "metadata": {
        "id": "VhTTCxldrzWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "41z_vlqMvYJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "CdoUdNaotJwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm\n"
      ],
      "metadata": {
        "id": "UF8-NuJssYyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "id": "khY4Bpqes0A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Jbszt9ZoqtV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Arquivos no diretório atual:\", os.listdir())\n"
      ],
      "metadata": {
        "id": "VLNsh3zwqQ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "BLUESKY_APP_USER = 'cocorolini.bsky.social'\n",
        "BLUESKY_APP_PASS='3ovj-gbnh-trwd-k66r'\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "YYunxEJnqSs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py search AQUECIMENTOGLOBAL --sort latest --limit 90\n"
      ],
      "metadata": {
        "id": "NcKekFBqqVM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# Defina o caminho correto para o arquivo CSV\n",
        "file_path = '/content/data/search_results_AQUECIMENTOGLOBAL_2024_09_16.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Cti5hDo8qkqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo de língua portuguesa\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Função de pré-processamento usando spaCy\n",
        "def preprocess_text_spacy(text_at):\n",
        "    # Aplicar o modelo spaCy ao texto\n",
        "    doc = nlp(text_at)\n",
        "    # Extrair palavras lematizadas e remover stopwords e pontuação\n",
        "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Aplicar pré-processamento ao DataFrame\n",
        "df['text_at'] = df['text'].apply(preprocess_text_spacy)  # Substitua 'post_content' pelo nome real da coluna\n"
      ],
      "metadata": {
        "id": "LIH2YhaqsE3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'text_at']].head())\n"
      ],
      "metadata": {
        "id": "NhrqBK5lsM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data/search_results_AQUECIMENTOGLOBAL_2024_09_16.csv')\n"
      ],
      "metadata": {
        "id": "o-zAyb26suIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pYGeAdSOzbC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "c28hT73jssTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "N2iqZ1xqtD43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "gafzyxqWz3eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"reply_count\", \"author_display_name\", \"author_handle\", \"indexed_at\", \"cid\", \"uri\", \"created_at\", \"repost_count\", \"like_count\"]\n",
        "df = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Exibir DataFrame após remover colunas\n",
        "print(\"\\nDepois:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "HfBNyuQhusFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "PE0hXhj6y4vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels =\n",
        "[0,0,0,1,1,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,\n",
        "1,0,1,0,0,0,0,0,0,1,\n",
        "0,0,0,0,0,0,0,0,1,1,\n",
        "1,0,0,0,1,0,0,0,0,0,\n",
        "0,0,0,0,0,0,0,0,0,0,\n",
        "0,0,1,1,0,0,0,0,1,0,\n",
        "1,0,0,0,0,0,0,0,0]\n"
      ],
      "metadata": {
        "id": "pbEnW7fT01ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYQlyriG0UbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenização\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Supondo que você já tenha um DataFrame `df` com as colunas 'text' e 'label'\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128  # Defina um tamanho máximo para padding e truncamento\n",
        "    )\n",
        "\n",
        "# Supondo um DataFrame com mais amostras\n",
        "df = pd.DataFrame({\n",
        "    'text': [\"Seu texto aqui\"] * 100,  # Adicione textos suficientes\n",
        "    'label': [0] * 100  # Adicione rótulos suficientes\n",
        "})\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Dividindo os dados em treino e validação\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "# Carregar métricas\n",
        "metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "# Definir os parâmetros de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    remove_unused_columns=False  # Adicionando esta linha para evitar erros de coluna\n",
        ")\n",
        "\n",
        "# Inicializar o modelo BERT\n",
        "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
        "\n",
        "# Treinamento\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "uVsHyDxltpqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}