{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcmeqTunohXsxukGKuBm+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4GfZThBF5dw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "  def _init_(self):\n",
        "    self.dicionario = {}\n",
        "    self.dicionario_inverso = {}\n",
        "    self.token_desconhecido = 1\n",
        "    self.token_padding = 0\n",
        "    self.max_len = 0\n",
        "\n",
        "  def fit(self, dataset):\n",
        "    indice = 2\n",
        "\n",
        "    for texto in dataset:\n",
        "      palavras = texto.split(' ')\n",
        "      self.max_len = len(palavras) if len(palavras) > self.max_len else self.max_len\n",
        "      for palavra in palavras:\n",
        "        if palavra not in self.dicionario:\n",
        "          self.dicionario[palavra] = indice\n",
        "          indice += 1\n",
        "\n",
        "    self.dicionario_inverso = {\n",
        "        valor : chave for chave, valor in dicionario.items()\n",
        "\n",
        "    }\n",
        "\n",
        "  def encode(self, texto):\n",
        "    tokens = texto.split('')\n",
        "    texto_tokenizado = []\n",
        "    for token in tokens:\n",
        "      if token not in self.dicionario.keys():\n",
        "        texto_tokenizado.append(self.token_desconhecido)\n",
        "      else:\n",
        "        texto_tokenizado.append(self.dicionario[token])\n",
        "    #from keras.preprocessing.sequence import pad_sequences\n",
        "    #texto_tokenizado = pad_sequences([texto_tokenizado], maxlen=self.max_len, padding='post')\n",
        "    for _ in range(self.max_len - len(texto_tokenizado)):\n",
        "      texto_tokenizado.append(self.token_padding)\n",
        "    return texto_tokenizado\n",
        "\n",
        "\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    texto = []\n",
        "    for token in tokens:\n",
        "      if token == self.token_padding:\n",
        "        texto.append('PAD')\n",
        "      elif token not in self.dicionario_inverso:\n",
        "        texto.append(self.token_desconhecido)\n",
        "      else:\n",
        "        texto.append(self.dicionario_inverso[token])\n",
        "\n",
        "    return ''.join(texto)\n",
        "\n",
        "  def save(self):\n",
        "    with open('dicionario.json', 'w') as f:\n",
        "      json.dump(self.dicionario, f)\n",
        "    with open('dicionario_inverso,json', 'w') as f:\n",
        "      json.dump(self.dicionario_inverso, f)\n",
        "\n",
        "    #salvando o max_len\n",
        "    with open('max_len.json', 'w') as f:\n",
        "      json.dump(self.max_len, f)\n",
        "\n",
        "\n",
        "  def load(self):\n",
        "    with open('dicionario.json', 'r') as f:\n",
        "      self.dicionario = json.load(f)\n",
        "    with open ('dicionario_inverson.json', 'r') as f:\n",
        "      self.dicionario_inverso = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YizWNn-MvuCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPy59IUllzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzIj4F9icuQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "  \"bom celular\",\n",
        "  \"sol, choveu durante o dia\",\n",
        "  \"bom dia, n√£o gostei disso\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(dataset)\n",
        "tokenized_dataset = [tokenizer.encode(x) for x in dataset]\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "7msbktuWTo96",
        "outputId": "24581403-72f6-446d-f06c-94c09ce9f153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tokenizer' object has no attribute 'max_len'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-82855704a70c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-69f259b89483>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtexto\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mpalavras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalavras\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalavras\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpalavras\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdicionario\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'max_len'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "quais camadas a rede tem?"
      ],
      "metadata": {
        "id": "wTH42rk_m8Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def _init_(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self)._init_()\n",
        "    self.input = nn.Linear(input_size, hidden_size)\n",
        "    self.hidden = nn.Linear(hidden_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    input = self.input(input)\n",
        "    if hidden is None:\n",
        "      hidden = self.initHidden()\n",
        "    hidden = self.hidden(hidden)\n",
        "    hidden = nn.Tanh()(hidden+input)\n",
        "    output = self.output(hidden)\n",
        "    return output, hidden\n",
        "\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jFx091mcm-2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "6YM_mD4VtdKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.input = nn.Linear(input_size, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    self.ft = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.it = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.ct = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.ot = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()  # Corrected typo (should be 'sigmoid')\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, input, state=None):\n",
        "    if state is None:\n",
        "      hidden, cell = self.initHidden()\n",
        "    else:\n",
        "      hidden, cell = state\n",
        "\n",
        "    input = self.input(input)\n",
        "\n",
        "    ft = torch.cat((hidden, input), 1)\n",
        "    ft = self.sigmoid(self.ft(ft))\n",
        "\n",
        "    it = torch.cat((hidden, input), 1)\n",
        "    it = self.sigmoid(self.it(it))\n",
        "\n",
        "    ct = torch.cat((hidden, input), 1)\n",
        "    ct = self.tanh(self.ct(ct))\n",
        "\n",
        "    ct = ft * cell + it * ct\n",
        "\n",
        "    ot = torch.cat((hidden, input), 1)\n",
        "    ot = self.sigmoid(self.ot(ot))\n",
        "\n",
        "    ht = ot * self.tanh(ct)\n",
        "    output = self.output(ht)\n",
        "\n",
        "    return output, [ht, ct]\n",
        "\n",
        "  def initHidden(self):\n",
        "    hidden_state = torch.zeros(1, self.hidden_size)\n",
        "    cell_state = torch.zeros(1, self.hidden_size)  # Corrected typo (torch.zeros)\n",
        "    return hidden_state, cell_state\n"
      ],
      "metadata": {
        "id": "weByPgHxtnry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def _init_(self, input_size, hidden_size):\n",
        "    super(GRU, self)._init_()\n",
        "    self.input = nn.Linear(input_size, hidden_size)\n",
        "    self,output = nn.Linaer(hidden_size, output_size)\n",
        "\n",
        "    self.zt = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.rt = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.ht = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    self.sigmoide = nn.Sigmoide()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    if hidden is None:\n",
        "      hidden = self.initHid\n",
        "\n",
        "    input = self.input(input)\n",
        "\n",
        "    zt = torch.cat((hidden, input), 1)\n",
        "    zt = self.sigmoide(self.zt(zt))\n",
        "\n",
        "    rt = torch.cat((hidden, input), 1)\n",
        "    rt = self.sigmoide(self.rt(rt))\n",
        "\n",
        "    ht = torch.cat((hidden, input), 1)\n",
        "    ht = self.tanh(self.ht(ht))\n",
        "\n",
        "    ht = (1 - zt) * hidden * zt * ht\n",
        "    return output, ht\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)"
      ],
      "metadata": {
        "id": "MVMFN-s73kdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
        "        )\n",
        "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.output = nn.Linear(dim_model, num_tokens)  # Changed for correct output\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "\n",
        "        src = src.permute(1, 0, 2)  # Permute for sequence-first format\n",
        "        tgt = tgt.permute(1, 0, 2)\n",
        "\n",
        "        output_transformer = self.transformer(\n",
        "            src,\n",
        "            tgt,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_pad_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask,\n",
        "        )\n",
        "        output = self.output(output_transformer)  # Apply output layer\n",
        "        return output\n",
        "\n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        mask = torch.tril(torch.ones(size, size))\n",
        "        mask = mask.masked_fill(mask == 0, float(\"-inf\"))  # Corrected typo \"mak\" to \"mask\"\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0))  # Changed float(0,0) to float(0.0)\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "0Jk_0mChy6de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(\n",
        "    num_tokens=len(tokenizer.dicionario.keys()) + 4,\n",
        "    dim_model=32,\n",
        "    num_heads=2,\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    dropout_p=0.1\n",
        ").to(device)\n",
        "opt = torch.optim.SGD(model.parametrs(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model"
      ],
      "metadata": {
        "id": "-Aa6f6dawUmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8b566b79-40e7-4404-ecb0-13853ab5242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tokenizer' object has no attribute 'dicionario'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5d89171b00db>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = Transformer(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnum_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdicionario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdim_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'dicionario'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for _ in range(1):\n",
        "  for sentence in tokenized_dataset:\n",
        "    x, y = sentence, sentence\n",
        "    x, y = torch.tensor(x).to(device)[None], torch.tensor(y).to(device)[None]\n",
        "\n",
        "    y_input = y[:, :-1]\n",
        "    y_expected = y[:, 1:]\n",
        "\n",
        "    tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "\n",
        "    pred = model(x, y, tgt_mask)\n",
        "\n",
        "    pred = prd.permute(1, 2, 0)\n",
        "    loss_fr(pred, y_expected)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "n8bOSKBqxaU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "219a1ee1-3a6b-4f55-9e89-37d795e2d0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-96869c36cca9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0T0YmIjrzsN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(5,5))\n",
        "mask"
      ],
      "metadata": {
        "id": "51CF1URvrxhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import dataloader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "r9n2aP_vPhyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "zQUbShJrUdXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "MVs6jU3DUf2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(pl.LightningModule):\n",
        "  def _init_(self):\n",
        "    super()._init_()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 *5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    self.train_acc = pl.metrics.Accuracy()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    inputs, labels = batch\n",
        "    pred = self.forward(inputs)\n",
        "\n",
        "    loss = self.criterion(pred, labels)\n",
        "    return loss\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(F.relu(x))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flattenn(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jlw5ixR1UmKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARDataMModule(pl.LightningDataModule):\n",
        "  def _init_(self, batch_size=128, transform=None):\n",
        "    super()._init_()\n",
        "    self.batch_size = batch_size\n",
        "    self.transform = transform\n",
        "\n",
        "  def prepare_data(self):\n",
        "    CIFAR10(root='data_train', train=True, download=True)\n",
        "    CIFAR10(root='data_train', train=False, download=True)\n",
        "\n",
        "  def setup(self, stage):\n",
        "    training_dataset = CIFAR10('data_train', download=False, train=True, transform = self.transform)\n",
        "    test_dataset = CIFAR10('data_test', download=False, train=False, transform = self.transform)\n",
        "    train, eval = random_split(training_dataset, [\n",
        "        int(len(training_dataset) * 0.7),\n",
        "        int(len(training_dataset) * 0.3)\n",
        "\n",
        "    ])\n",
        "\n",
        "    self.train_dataset = train\n",
        "    self.eval_dataset = eval\n",
        "    self.test_dataset = test_dataset\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "d_fC7K_KUoew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "transform = transforms.ToTensor()\n",
        "cifar_dm = CIFARDataModule(batch_size=128, transform=transform)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=2, gpus=0)\n",
        "trainer.fit(model, cifar_dm)"
      ],
      "metadata": {
        "id": "yF15XT0xUqEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}