{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP8iyIiHc6Hwax1jzG0T6WP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolineoliveira994/100-days-of-code-python/blob/main/regularizacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regulariza√ß√£o: adiciona ruido ou preferencia de comporta,ento do modelo para evtiar super-ajuste\n",
        "\n",
        "L1\n",
        "\n",
        "Sparsidade dos Coeficientes: A regulariza√ß√£o L1 tende a reduzir alguns coeficientes a exatamente zero, o que leva a modelos mais simples e interpret√°veis. Isso pode ser √∫til para a sele√ß√£o de caracter√≠sticas, j√° que o modelo naturalmente ignora caracter√≠sticas menos importantes.\n",
        "\n",
        "Preven√ß√£o de Overfitting: Ao adicionar a penalidade ao valor absoluto dos coeficientes, a regulariza√ß√£o L1 reduz a complexidade do modelo, ajudando a prevenir o overfitting.\n",
        "\n",
        "L2\n",
        "\n",
        "Utilizada em modelos de regress√£o para prevenir o overfitting e melhorar a generaliza√ß√£o do modelo para novos dados. A regulariza√ß√£o L2 adiciona uma penalidade ao quadrado dos coeficientes do modelo na fun√ß√£o de custo.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UzsQQ-vtEv5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAzZEanHArAP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir o n√∫mero de amostras e caracter√≠sticas\n",
        "n_samples = 100\n",
        "n_features = 5\n",
        "\n",
        "# Gerar caracter√≠sticas aleat√≥rias\n",
        "np.random.seed(42)  # Para reprodutibilidade\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Gerar coeficientes verdadeiros\n",
        "true_coefficients = np.array([1.5, -2.0, 0.0, 3.0, -1.0])\n",
        "\n",
        "# Gerar a vari√°vel alvo com algum ru√≠do\n",
        "noise = np.random.randn(n_samples)\n",
        "y = X @ true_coefficients + noise\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criar o modelo de regress√£o Ridge com o par√¢metro de regulariza√ß√£o alpha\n",
        "ridge = Ridge(alpha=1.0)\n",
        "\n",
        "# Ajustar o modelo aos dados de treinamento\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Prever nos dados de teste\n",
        "y_pred = ridge.predict(X_test)\n",
        "\n",
        "# Calcular o erro quadr√°tico m√©dio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Coeficientes do modelo\n",
        "print(f'Coeficientes: {ridge.coef_}')\n",
        "\n",
        "# Plotar os coeficientes verdadeiros vs coeficientes estimados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(true_coefficients, 'o', label='Coeficientes Verdadeiros')\n",
        "plt.plot(ridge.coef_, 'x', label='Coeficientes Estimados')\n",
        "plt.legend()\n",
        "plt.title('Coeficientes Verdadeiros vs Estimados')\n",
        "plt.xlabel('√çndice do Coeficiente')\n",
        "plt.ylabel('Valor do Coeficiente')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04tO2dqmEvdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante o treinamento, o Dropout atua em cada camada da rede neural (geralmente nas camadas totalmente conectadas). Em cada itera√ß√£o, cada neur√¥nio tem uma probabilidade ùëù\n",
        "p de ser \"desligado\" (ou seja, ser temporariamente removido da rede). Isso significa que as ativa√ß√µes desses neur√¥nios n√£o s√£o propagadas para a pr√≥xima camada durante essa itera√ß√£o. Durante a fase de infer√™ncia (ou seja, quando a rede √© usada para fazer previs√µes), todos os neur√¥nios s√£o usados, mas suas ativa√ß√µes s√£o escaladas pelo fator ùëù\n",
        "p para manter a consist√™ncia.\n",
        "\n",
        "comi√™ de redes ultilizando apenas a base\n",
        "\n",
        "batch normalization (bugs *\n",
        "\n",
        "melhora o fluxo de gradientes na rede\n",
        "deixa o treinamento de redes profundas muito mais facil\n",
        "permite o uso de taxas de aprendizado maiores\n",
        "redes se torncam mais robustas a inicializacao ruins\n",
        "age como regularizador durante treinamento\n",
        "\n",
        "\n",
        "adiconar ruido, restricaoi preferencia do modelo para evitar super ajuste\n",
        "\n",
        "mudar fubncao de custo: regularizacao l1/l2\n",
        "\n"
      ],
      "metadata": {
        "id": "SnfoRfUPRvTt"
      }
    }
  ]
}